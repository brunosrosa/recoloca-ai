# -*- coding: utf-8 -*-
"""
Retriever RAG para o Sistema Recoloca.ai

Este m√≥dulo √© respons√°vel por realizar consultas no √≠ndice FAISS,
retornando os documentos mais relevantes para uma query espec√≠fica.

Autor: @AgenteM_DevFastAPI
Vers√£o: 1.0
Data: Junho 2025
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import faiss
import numpy as np

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from .constants import (
        FAISS_INDEX_DIR,
        FAISS_INDEX_FILE,
        FAISS_DOCUMENTS_FILE,
        FAISS_METADATA_FILE,
        FAISS_EMBEDDINGS_FILE,
        FAISS_MAPPING_FILE,
        EMBEDDING_MODEL_NAME,
        USE_GPU,
        DEFAULT_TOP_K,
        MIN_SIMILARITY_SCORE,
        LOGS_DIR,
        STATUS_MESSAGES,
        RESULT_TEMPLATE
    )
except ImportError:
    from constants import (
        FAISS_INDEX_DIR,
        FAISS_INDEX_FILE,
        FAISS_DOCUMENTS_FILE,
        FAISS_METADATA_FILE,
        FAISS_EMBEDDINGS_FILE,
        FAISS_MAPPING_FILE,
        EMBEDDING_MODEL_NAME,
        USE_GPU,
        DEFAULT_TOP_K,
        MIN_SIMILARITY_SCORE,
        LOGS_DIR,
        STATUS_MESSAGES,
        RESULT_TEMPLATE
    )

try:
    from .embedding_model import initialize_embedding_model, get_embedding_manager
except ImportError:
    from embedding_model import initialize_embedding_model, get_embedding_manager

try:
    from .pytorch_gpu_retriever import PyTorchGPURetriever
except ImportError:
    from pytorch_gpu_retriever import PyTorchGPURetriever

try:
    from .pytorch_optimizations import OptimizedPyTorchRetriever
except ImportError:
    from pytorch_optimizations import OptimizedPyTorchRetriever

# Importar sistema de m√©tricas
try:
    from rag_metrics_integration import track_query, track_batch_query, metrics_integration
    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False
    # Decoradores vazios se m√©tricas n√£o dispon√≠veis
    def track_query(func):
        return func
    def track_batch_query(func):
        return func

# Configurar logging
import sys
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger(__name__)

# Criar handler para arquivo de log
LOGS_DIR.mkdir(parents=True, exist_ok=True)
file_handler = logging.FileHandler(LOGS_DIR / "rag_retriever.log")
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)

# Constantes
MAX_TOP_K = 100
CACHE_MAX_SIZE = 1000

def _detect_gpu_compatibility() -> bool:
    """
    Detecta se a GPU √© compat√≠vel com FAISS-GPU.
    
    Returns:
        bool: True se compat√≠vel com FAISS-GPU, False caso contr√°rio
    """
    try:
        import torch
        if not torch.cuda.is_available():
            logger.info("CUDA n√£o dispon√≠vel - usando CPU")
            return False
            
        # Verificar se √© RTX 2060 ou similar (problemas conhecidos com FAISS-GPU)
        gpu_name = torch.cuda.get_device_name(0).lower()
        incompatible_gpus = ['rtx 2060', 'gtx 1660', 'gtx 1650']
        
        for incompatible in incompatible_gpus:
            if incompatible in gpu_name:
                logger.info(f"GPU {gpu_name} detectada - usando PyTorch em vez de FAISS-GPU")
                return False
                
        # Tentar importar FAISS-GPU
        try:
            import faiss
            if hasattr(faiss, 'StandardGpuResources'):
                logger.info(f"GPU {gpu_name} compat√≠vel com FAISS-GPU")
                return True
        except Exception as e:
            logger.warning(f"FAISS-GPU n√£o dispon√≠vel: {e}")
            
        return False
        
    except Exception as e:
        logger.warning(f"Erro na detec√ß√£o de GPU: {e}")
        return False

class SearchResult:
    """
    Classe para representar um resultado de busca.
    """
    
    def __init__(self, content: str, metadata: Dict[str, Any], score: float, rank: int):
        self.content = content
        self.metadata = metadata
        self.score = score
        self.rank = rank
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Converte o resultado para dicion√°rio.
        
        Returns:
            Dict: Representa√ß√£o em dicion√°rio do resultado
        """
        return {
            "content": self.content,
            "metadata": self.metadata,
            "score": self.score,
            "rank": self.rank
        }
    
    def format_result(self) -> str:
        """
        Formata o resultado para exibi√ß√£o.
        
        Returns:
            str: Resultado formatado
        """
        return RESULT_TEMPLATE.format(
            source=self.metadata.get("source", "Desconhecido"),
            section=self.metadata.get("section", "main"),
            score=self.score,
            content=self.content[:500] + "..." if len(self.content) > 500 else self.content
        )
    
    def __str__(self) -> str:
        return f"SearchResult(rank={self.rank}, score={self.score:.3f}, source={self.metadata.get('source', 'Unknown')})"
    
    def __repr__(self) -> str:
        return self.__str__()

class RAGRetriever:
    """
    Classe principal para recupera√ß√£o de documentos usando FAISS ou PyTorch.
    Implementa fallback inteligente baseado na compatibilidade da GPU.
    """
    
    def __init__(self, force_cpu: bool = False, force_pytorch: bool = False, use_optimizations: bool = False, cache_enabled: bool = True, batch_size: int = 5):
        """
        Inicializa o retriever.
        
        Args:
            force_cpu: Se True, for√ßa o uso de CPU mesmo com GPU dispon√≠vel
            force_pytorch: Se True, for√ßa o uso do PyTorchGPURetriever
            use_optimizations: Usar vers√£o otimizada com cache e batch processing
            cache_enabled: Habilitar cache persistente (apenas com otimiza√ß√µes)
            batch_size: Tamanho do batch para processamento (apenas com otimiza√ß√µes)
        """
        self.force_cpu = force_cpu
        self.force_pytorch = force_pytorch
        self.use_optimizations = use_optimizations
        self.cache_enabled = cache_enabled
        self.batch_size = batch_size
        self.embedding_manager = None
        
        # Detectar qual backend usar
        self.use_pytorch = self._should_use_pytorch()
        
        # Inicializar backend apropriado
        if self.use_pytorch:
            if use_optimizations:
                logger.info("Usando OptimizedPyTorchRetriever")
                self.pytorch_retriever = OptimizedPyTorchRetriever(
                    force_cpu=force_cpu,
                    cache_enabled=cache_enabled,
                    batch_size=batch_size
                )
            else:
                logger.info("Usando PyTorchGPURetriever")
                self.pytorch_retriever = PyTorchGPURetriever(force_cpu=force_cpu)
            self.index = None
        else:
            logger.info("üìä Usando FAISS")
            self.pytorch_retriever = None
            self.index = None
            
        self.documents = []
        self.metadata = []
        self.index_metadata = {}
        self.is_loaded = False
        
        # Cache de consultas
        self._query_cache: Dict[str, List[SearchResult]] = {}
        self._cache_max_size = 100
        
    def _should_use_pytorch(self) -> bool:
        """
        Determina se deve usar PyTorch em vez de FAISS.
        
        Returns:
            bool: True se deve usar PyTorch
        """
        if self.force_pytorch:
            return True
            
        if self.force_cpu:
            return False
            
        # Usar PyTorch se GPU n√£o for compat√≠vel com FAISS
        return not _detect_gpu_compatibility()
    
    def initialize(self) -> bool:
        """
        Inicializa o modelo de embedding e o backend apropriado.
        
        Returns:
            bool: True se inicializado com sucesso
        """
        try:
            logger.info("Inicializando modelo de embedding...")
            
            if self.use_pytorch:
                # Inicializar PyTorchGPURetriever
                success = self.pytorch_retriever.initialize()
                if not success:
                    logger.error("Falha ao inicializar PyTorchGPURetriever")
                    return False
                logger.info("‚úÖ PyTorchGPURetriever inicializado com sucesso")
            else:
                # Inicializar EmbeddingManager para FAISS
                success = initialize_embedding_model(force_cpu=self.force_cpu)
                if success:
                    self.embedding_manager = get_embedding_manager()
                    logger.info("‚úÖ Modelo de embedding inicializado")
                else:
                    logger.error("‚ùå Falha ao inicializar modelo de embedding")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"Erro na inicializa√ß√£o: {str(e)}")
            return False
    
    def load_index(self) -> bool:
        """
        Carrega o √≠ndice do disco (FAISS ou PyTorch).
        
        Returns:
            bool: True se carregado com sucesso
        """
        try:
            logger.info(f"DEBUG: use_pytorch = {self.use_pytorch}")
            logger.info(f"DEBUG: pytorch_retriever = {self.pytorch_retriever}")
            
            if self.use_pytorch:
                # Carregar usando PyTorchGPURetriever
                logger.info("Carregando √≠ndice PyTorch...")
                
                if not self.pytorch_retriever:
                    logger.error("‚ùå pytorch_retriever n√£o est√° inicializado")
                    return False
                    
                success = self.pytorch_retriever.load_index()
                logger.info(f"DEBUG: pytorch_retriever.load_index() retornou: {success}")
                
                if success:
                    # Sincronizar dados com o retriever principal
                    # Para OptimizedPyTorchRetriever, acessar atrav√©s do base_retriever
                    if hasattr(self.pytorch_retriever, 'base_retriever'):
                        # √â um OptimizedPyTorchRetriever
                        self.documents = self.pytorch_retriever.base_retriever.documents
                        self.metadata = self.pytorch_retriever.base_retriever.metadata
                        self.index_metadata = self.pytorch_retriever.base_retriever.index_metadata
                    else:
                        # √â um PyTorchGPURetriever direto
                        self.documents = self.pytorch_retriever.documents
                        self.metadata = self.pytorch_retriever.metadata
                        self.index_metadata = self.pytorch_retriever.index_metadata
                    
                    self.is_loaded = True
                    logger.info(f"‚úÖ √çndice PyTorch carregado: {len(self.documents)} documentos")
                    return True
                else:
                    logger.error("‚ùå Falha ao carregar √≠ndice PyTorch")
                    return False
            else:
                # Carregar usando FAISS (c√≥digo original)
                logger.info("Carregando √≠ndice FAISS...")
                return self._load_faiss_index()
                
        except Exception as e:
            logger.error(f"Erro ao carregar √≠ndice: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
            
    def _load_faiss_index(self) -> bool:
        """
        Carrega o √≠ndice FAISS do disco (m√©todo original).
        
        Returns:
            bool: True se carregado com sucesso
        """
        try:
            # Verificar se os arquivos existem
            index_path = FAISS_INDEX_DIR / FAISS_INDEX_FILE
            documents_path = FAISS_INDEX_DIR / FAISS_DOCUMENTS_FILE
            metadata_path = FAISS_INDEX_DIR / FAISS_METADATA_FILE
            
            if not all([index_path.exists(), documents_path.exists(), metadata_path.exists()]):
                logger.error("Arquivos do √≠ndice n√£o encontrados. Execute a indexa√ß√£o primeiro.")
                return False
            
            logger.info("Carregando √≠ndice FAISS...")
            start_time = time.time()
            
            # Carregar √≠ndice FAISS usando diret√≥rio tempor√°rio para evitar problemas com Unicode
            import tempfile
            import shutil
            import os
            from uuid import uuid4
            
            # Usar diret√≥rio tempor√°rio seguro
            temp_dir = "C:\\Temp" if os.name == "nt" else "/tmp"
            if not Path(temp_dir).exists():
                Path(temp_dir).mkdir(parents=True, exist_ok=True)
            
            with tempfile.TemporaryDirectory(dir=temp_dir) as temp_path:
                temp_file_path = Path(temp_path) / f"faiss_index_{uuid4().hex}.bin"
                shutil.copy2(str(index_path), str(temp_file_path))
                self.index = faiss.read_index(str(temp_file_path))
            
            logger.info(f"√çndice FAISS carregado: {self.index.ntotal} vetores")
            
            # Carregar documentos
            with open(documents_path, 'r', encoding='utf-8-sig') as f:
                self.documents = json.load(f)
            logger.info(f"Documentos carregados: {len(self.documents)} chunks")
            
            # Carregar metadados
            with open(metadata_path, 'r', encoding='utf-8-sig') as f:
                index_data = json.load(f)
                
                # Verificar se √© uma lista direta ou um dicion√°rio
                if isinstance(index_data, list):
                    # Formato novo: lista direta de metadados
                    self.metadata = index_data
                    self.index_metadata = {
                        "documents_metadata": index_data,
                        "total_documents": len(index_data),
                        "format_version": "2.0"
                    }
                elif isinstance(index_data, dict):
                    # Formato antigo: dicion√°rio com chave 'documents_metadata'
                    self.index_metadata = index_data
                    self.metadata = index_data.get("documents_metadata", [])
                else:
                    logger.error(f"Formato de metadados inv√°lido: {type(index_data)}")
                    return False
            logger.info(f"Metadados carregados: {len(self.metadata)} entradas")
            
            # Verificar consist√™ncia
            if len(self.documents) != len(self.metadata):
                logger.warning(f"Inconsist√™ncia: {len(self.documents)} documentos vs {len(self.metadata)} metadados")
            
            if self.index.ntotal != len(self.documents):
                logger.warning(f"Inconsist√™ncia: {self.index.ntotal} vetores vs {len(self.documents)} documentos")
            
            elapsed_time = time.time() - start_time
            logger.info(f"√çndice carregado em {elapsed_time:.2f}s")
            
            self.is_loaded = True
            return True
            
        except Exception as e:
            logger.error(f"Erro ao carregar √≠ndice FAISS: {str(e)}")
            return False
    
    @track_query
    def search(self, query: str, top_k: int = DEFAULT_TOP_K, 
                min_score: float = MIN_SIMILARITY_SCORE, 
                category_filter: Optional[str] = None) -> List[SearchResult]:
        """
        Realiza busca sem√¢ntica no √≠ndice.
        
        Args:
            query: Consulta de busca
            top_k: N√∫mero de resultados a retornar
            min_score: Score m√≠nimo de similaridade
            category_filter: Filtro por categoria (opcional)
            
        Returns:
            List[SearchResult]: Lista de resultados ordenados por relev√¢ncia
        """
        if not self.is_loaded:
            logger.error("√çndice n√£o carregado. Chame load_index() primeiro.")
            return []
        
        # Validar par√¢metros
        top_k = min(max(1, top_k), 100)  # MAX_TOP_K
        min_score = max(0.0, min(1.0, min_score))
        
        # Verificar cache
        cache_key = f"{query}_{top_k}_{min_score}_{category_filter}"
        cache_hit = False
        if cache_key in self._query_cache:
            cache_hit = True
            logger.debug(f"Resultado encontrado no cache para: {query[:50]}...")
            cached_results = self._query_cache[cache_key]
            # Adicionar informa√ß√£o de cache hit aos metadados
            for result in cached_results:
                if hasattr(result, 'metadata') and isinstance(result.metadata, dict):
                    result.metadata['cache_hit'] = True
            return cached_results
        
        try:
            logger.info(STATUS_MESSAGES["query_start"].format(query=query[:100]))
            start_time = time.time()
            
            if self.use_pytorch:
                # Usar PyTorchGPURetriever
                pytorch_results = self.pytorch_retriever.search(
                    query=query,
                    top_k=top_k,
                    min_score=min_score
                )
                
                # Converter para SearchResult e aplicar filtro de categoria
                results = []
                for pytorch_result in pytorch_results:
                    # Aplicar filtro de categoria se especificado
                    if category_filter and pytorch_result.metadata.get("category") != category_filter:
                        continue
                        
                    # Converter para o formato SearchResult do RAGRetriever
                    result = SearchResult(
                        content=pytorch_result.content,
                        metadata=pytorch_result.metadata,
                        score=pytorch_result.score,
                        rank=pytorch_result.rank
                    )
                    results.append(result)
                    
                    if len(results) >= top_k:
                        break
                        
            else:
                # Usar FAISS (c√≥digo original)
                results = self._search_faiss(query, top_k, min_score, category_filter)
            
            elapsed_time = time.time() - start_time
            logger.info(STATUS_MESSAGES["query_complete"].format(count=len(results)))
            logger.info(f"Busca realizada em {elapsed_time:.3f}s")
            
            # Adicionar informa√ß√£o de cache miss aos metadados
            for result in results:
                if hasattr(result, 'metadata') and isinstance(result.metadata, dict):
                    result.metadata['cache_hit'] = False
            
            # Cache do resultado
            self._cache_result(cache_key, results)
            
            return results
            
        except Exception as e:
            logger.error(STATUS_MESSAGES["query_error"].format(error=str(e)))
            return []
    
    @track_batch_query
    def search_batch(self, queries: List[str], top_k: int = DEFAULT_TOP_K, 
                    min_score: float = MIN_SIMILARITY_SCORE, 
                    category_filter: Optional[str] = None) -> List[List[SearchResult]]:
        """
        Realiza busca sem√¢ntica em lote para m√∫ltiplas consultas.
        
        Args:
            queries: Lista de consultas de busca
            top_k: N√∫mero de resultados a retornar por consulta
            min_score: Score m√≠nimo de similaridade
            category_filter: Filtro por categoria (opcional)
            
        Returns:
            List[List[SearchResult]]: Lista de listas de resultados para cada consulta
        """
        if not queries:
            return []
        
        logger.info(f"Iniciando busca em lote para {len(queries)} consultas")
        start_time = time.time()
        
        results = []
        cache_hits = 0
        
        try:
            # Processar cada consulta
            for i, query in enumerate(queries):
                query_results = self.search(query, top_k, min_score, category_filter)
                
                # Contar cache hits
                for result in query_results:
                    if hasattr(result, 'metadata') and isinstance(result.metadata, dict):
                        if result.metadata.get('cache_hit', False):
                            cache_hits += 1
                        # Adicionar informa√ß√£o de lote
                        result.metadata['batch_index'] = i
                        result.metadata['batch_size'] = len(queries)
                
                results.append(query_results)
            
            elapsed_time = time.time() - start_time
            total_results = sum(len(r) for r in results)
            cache_hit_rate = cache_hits / total_results if total_results > 0 else 0
            
            logger.info(f"Busca em lote conclu√≠da: {len(queries)} consultas, "
                       f"{total_results} resultados, {elapsed_time:.3f}s, "
                       f"cache hit rate: {cache_hit_rate:.2%}")
            
            return results
            
        except Exception as e:
            logger.error(f"Erro durante busca em lote: {e}")
            return [[] for _ in queries]
            
    def _search_faiss(self, query: str, top_k: int, min_score: float, 
                     category_filter: Optional[str] = None) -> List[SearchResult]:
        """
        Realiza busca usando FAISS (m√©todo original).
        
        Args:
            query: Consulta de busca
            top_k: N√∫mero de resultados a retornar
            min_score: Score m√≠nimo de similaridade
            category_filter: Filtro por categoria (opcional)
            
        Returns:
            List[SearchResult]: Lista de resultados ordenados por relev√¢ncia
        """
        if not self.embedding_manager:
            logger.error("Modelo de embedding n√£o inicializado.")
            return []
            
        # 1. Gerar embedding da consulta
        query_embedding = self.embedding_manager.embed_query(query)
        if query_embedding is None:
            logger.error("Falha ao gerar embedding da consulta")
            return []
        
        # 2. Buscar no √≠ndice FAISS
        query_vector = query_embedding.reshape(1, -1).astype('float32')
        scores, indices = self.index.search(query_vector, top_k * 2)  # Buscar mais para filtrar
        
        # 3. Processar resultados
        results = []
        for rank, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx == -1:  # √çndice inv√°lido
                continue
            
            if score < min_score:
                continue
            
            # Obter documento e metadados
            if idx >= len(self.documents) or idx >= len(self.metadata):
                logger.warning(f"√çndice fora do range: {idx}")
                continue
            
            document = self.documents[idx]
            metadata = self.metadata[idx]
            
            # Aplicar filtro de categoria se especificado
            if category_filter and metadata.get("category") != category_filter:
                continue
            
            result = SearchResult(
                content=document,
                metadata=metadata,
                score=float(score),
                rank=rank + 1
            )
            
            results.append(result)
            
            # Parar se j√° temos resultados suficientes
            if len(results) >= top_k:
                break
        
        # 4. Reordenar por score (descendente)
        results.sort(key=lambda x: x.score, reverse=True)
        
        # 5. Atualizar ranks
        for i, result in enumerate(results):
            result.rank = i + 1
        
        return results
    
    def search_by_document(self, document_pattern: str, top_k: int = DEFAULT_TOP_K) -> List[SearchResult]:
        """
        Busca documentos por padr√£o no nome/caminho.
        
        Args:
            document_pattern: Padr√£o para buscar no nome do documento
            top_k: N√∫mero m√°ximo de resultados
            
        Returns:
            List[SearchResult]: Lista de resultados encontrados
        """
        if not self.is_loaded:
            logger.error("√çndice n√£o carregado.")
            return []
        
        try:
            results = []
            pattern_lower = document_pattern.lower()
            
            for idx, metadata in enumerate(self.metadata):
                source = metadata.get("source", "").lower()
                title = metadata.get("title", "").lower()
                
                if pattern_lower in source or pattern_lower in title:
                    if idx < len(self.documents):
                        result = SearchResult(
                            content=self.documents[idx],
                            metadata=metadata,
                            score=1.0,  # Score fixo para busca por documento
                            rank=len(results) + 1
                        )
                        results.append(result)
                        
                        if len(results) >= top_k:
                            break
            
            logger.info(f"Encontrados {len(results)} documentos para padr√£o: {document_pattern}")
            return results
            
        except Exception as e:
            logger.error(f"Erro na busca por documento: {str(e)}")
            return []
    
    def get_document_list(self, category: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Retorna lista de documentos indexados.
        
        Args:
            category: Filtro por categoria (opcional)
            
        Returns:
            List[Dict]: Lista de informa√ß√µes dos documentos
        """
        if not self.is_loaded:
            logger.error("√çndice n√£o carregado.")
            return []
        
        try:
            documents_info = []
            
            # Agrupar chunks por documento fonte
            doc_groups = {}
            for metadata in self.metadata:
                source = metadata.get("source", "unknown")
                doc_category = metadata.get("category", "general")
                
                # Aplicar filtro de categoria se especificado
                if category and doc_category != category:
                    continue
                
                if source not in doc_groups:
                    doc_groups[source] = {
                        "source": source,
                        "title": metadata.get("title", "Sem t√≠tulo"),
                        "category": doc_category,
                        "file_type": metadata.get("file_type", "unknown"),
                        "chunks_count": 0,
                        "last_updated": metadata.get("timestamp", "unknown")
                    }
                
                doc_groups[source]["chunks_count"] += 1
            
            documents_info = list(doc_groups.values())
            documents_info.sort(key=lambda x: x["source"])
            
            logger.info(f"Retornando informa√ß√µes de {len(documents_info)} documentos")
            return documents_info
            
        except Exception as e:
            logger.error(f"Erro ao obter lista de documentos: {str(e)}")
            return []
    
    def get_index_info(self) -> Dict[str, Any]:
        """
        Retorna informa√ß√µes sobre o √≠ndice carregado.
        
        Returns:
            Dict: Informa√ß√µes do √≠ndice
        """
        if not self.is_loaded:
            return {"loaded": False, "error": "√çndice n√£o carregado"}
        
        base_info = {
            "loaded": True,
            "backend": "PyTorch_Optimized" if (self.use_pytorch and self.use_optimizations) else ("PyTorch" if self.use_pytorch else "FAISS"),
            "total_documents": len(self.documents),
            "total_metadata": len(self.metadata),
            "embedding_model": self.index_metadata.get("embedding_model", "unknown"),
            "created_at": self.index_metadata.get("created_at", "unknown"),
            "index_dimension": self.index_metadata.get("index_dimension", 0),
            "chunk_size": self.index_metadata.get("chunk_size", 0),
            "chunk_overlap": self.index_metadata.get("chunk_overlap", 0),
            "cache_size": len(self._query_cache),
            "optimizations_enabled": self.use_optimizations
        }
        
        if self.use_pytorch:
            # Informa√ß√µes espec√≠ficas do PyTorch
            pytorch_info = self.pytorch_retriever.get_index_info()
            base_info.update({
                "total_vectors": pytorch_info.get("total_vectors", 0),
                "device": pytorch_info.get("device", "unknown"),
                "gpu_available": pytorch_info.get("gpu_available", False),
                "memory_usage": pytorch_info.get("memory_usage", "unknown")
            })
            
            # Adicionar estat√≠sticas se usando otimiza√ß√µes
            if self.use_optimizations and hasattr(self.pytorch_retriever, 'get_stats'):
                base_info["performance_stats"] = self.pytorch_retriever.get_stats()
        else:
            # Informa√ß√µes espec√≠ficas do FAISS
            base_info["total_vectors"] = self.index.ntotal if self.index else 0
            
        return base_info
    
    def get_backend_info(self) -> Dict[str, Any]:
        """Obt√©m informa√ß√µes sobre o backend atual."""
        import torch
        
        gpu_available = torch.cuda.is_available()
        recommended_backend = "pytorch" if gpu_available else "sklearn"
        
        backend_info = {
            "current_backend": "OptimizedPyTorch" if self.use_optimizations else "PyTorch" if self.force_pytorch else "Auto",
            "recommended_backend": recommended_backend,
            "gpu_available": gpu_available,
            "cuda_device_count": torch.cuda.device_count() if gpu_available else 0
        }
        
        if gpu_available:
            backend_info["gpu_name"] = torch.cuda.get_device_name(0)
            backend_info["gpu_memory_total"] = torch.cuda.get_device_properties(0).total_memory
        
        return backend_info
    
    def _cache_result(self, cache_key: str, results: List[SearchResult]) -> None:
        """
        Armazena resultado no cache.
        
        Args:
            cache_key: Chave do cache
            results: Resultados para cachear
        """
        # Limpar cache se estiver muito grande
        if len(self._query_cache) >= self._cache_max_size:
            # Remover entradas mais antigas (FIFO simples)
            oldest_key = next(iter(self._query_cache))
            del self._query_cache[oldest_key]
        
        self._query_cache[cache_key] = results
    
    def clear_cache(self) -> None:
        """
        Limpa o cache de consultas.
        """
        cache_size = len(self._query_cache)
        self._query_cache.clear()
        logger.info(f"Cache limpo. {cache_size} consultas removidas.")
    
    def reload_index(self) -> bool:
        """
        Recarrega o √≠ndice do disco.
        
        Returns:
            bool: True se recarregado com sucesso
        """
        logger.info("Recarregando √≠ndice...")
        self.is_loaded = False
        self.clear_cache()
        return self.load_index()

# Inst√¢ncia global do retriever (singleton pattern)
_retriever: Optional[RAGRetriever] = None

def get_retriever(force_cpu: bool = False, force_pytorch: bool = False) -> RAGRetriever:
    """
    Retorna a inst√¢ncia global do retriever.
    
    Args:
        force_cpu: Se True, for√ßa o uso de CPU
        force_pytorch: Se True, for√ßa o uso do PyTorchGPURetriever
        
    Returns:
        RAGRetriever: Inst√¢ncia do retriever
    """
    global _retriever
    
    if _retriever is None:
        _retriever = RAGRetriever(force_cpu=force_cpu, force_pytorch=force_pytorch)
    
    return _retriever

def initialize_retriever(force_cpu: bool = False, force_pytorch: bool = False) -> bool:
    """
    Inicializa o retriever global.
    
    Args:
        force_cpu: Se True, for√ßa o uso de CPU
        force_pytorch: Se True, for√ßa o uso do PyTorchGPURetriever
        
    Returns:
        bool: True se inicializado com sucesso
    """
    retriever = get_retriever(force_cpu=force_cpu, force_pytorch=force_pytorch)
    
    # Inicializar modelo de embedding
    if not retriever.initialize():
        return False
    
    # Carregar √≠ndice
    return retriever.load_index()

def search_documents(query: str, top_k: int = DEFAULT_TOP_K, 
                    min_score: float = MIN_SIMILARITY_SCORE,
                    category_filter: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Fun√ß√£o de conveni√™ncia para busca de documentos.
    
    Args:
        query: Consulta de busca
        top_k: N√∫mero de resultados
        min_score: Score m√≠nimo
        category_filter: Filtro por categoria
        
    Returns:
        List[Dict]: Resultados da busca
    """
    retriever = get_retriever()
    
    if not retriever.is_loaded:
        logger.error("Retriever n√£o inicializado")
        return []
    
    results = retriever.search(query, top_k, min_score, category_filter)
    return [result.to_dict() for result in results]

if __name__ == "__main__":
    # Teste do m√≥dulo
    print("üß™ Testando m√≥dulo retriever...")
    
    # Inicializar retriever
    success = initialize_retriever()
    
    if success:
        retriever = get_retriever()
        
        # Teste de consulta
        query = "Como implementar autentica√ß√£o no FastAPI?"
        results = retriever.search(query, top_k=3)
        
        print(f"\nüîç Consulta: {query}")
        print(f"üìä Resultados encontrados: {len(results)}")
        
        for result in results:
            print(f"\n{result.format_result()}")
        
        # Informa√ß√µes do √≠ndice
        info = retriever.get_index_info()
        print(f"\nüìà Info do √≠ndice: {info}")
        
    else:
        print("‚ùå Falha ao inicializar retriever")